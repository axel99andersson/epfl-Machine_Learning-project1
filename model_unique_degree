import numpy as np
from implementations import *
from preprocessing import *
from cross_validation import *

"""
Ändra lambdas och/eller degrees
"""
# Load Data
set_of_features = 2
y, X, idx = load_data(set_of_features)

# Transform Data
X = standardize_data(X, -999)
#y, X = create_even_data(y, X)
X = X[:10000]
y = y[:10000]
# Build Model (välj model, cross-validation för lambda och polynomgrad)
plot = False
degrees = [12,13,16]
features = [i for i in range(X.shape[1])]
print(features)
#lambdas = np.logspace(-11, -8, 5)
lambdas = [5.6234132519034906e-11]
k_fold = 5
seed = 12
k_indices = build_k_indices(y, k_fold, seed)
best_test_loss = float('inf')
best_model_accuracy = float('-inf')
best_degree = []
best_degree_features = []
model_accuracy = 0
model_accuracy_list = []
for feature in features:
    for degree in degrees:
        for lambda_ in lambdas:
            model_accuracy = 0
            for k in range(k_fold):
                [loss_tr, loss_te, model_accuracy_k] = cross_validation_one_feature(y, X, k_indices, k, lambda_, degree, feature)
                model_accuracy += model_accuracy_k

            model_accuracy /= k_fold
        if model_accuracy > best_model_accuracy:
            best_model_accuracy = model_accuracy
            best_degree = degree
    best_degree_features.append(best_degree)

print("-------------------------")
print("Best degree per feature", best_degree_features, "\nModel Accuracy:", best_model_accuracy, "%")
print("-------------------------")
print("Training Done")

# Predict
